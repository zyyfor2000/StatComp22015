---
title: "Introduction to StatComp22015"
author: "22015"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to StatComp22015}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

# Content {#Content}

1.  Overview [Jump](#Overview)

2.  Compare between R and R++ [Jump](#Compare)

3.  Simulation for the results in the paper [Jump](#Simulation)

# Overview {#Overview}

__StatComp22015__ is a simple R package developed to compare the performance of R and R++ (implemented through the R package _Rcpp_) for the 'Statistical Computing' course. Two functions are considered, namely, _gibbs_ (generate random nubers using Gibbs sampler) and _rwMetropolis_ (A random walk Metropolis sampler for t distribution). For each function, both R and Rcpp versions are produced. Namely _gibbsR_ and  _rwMetropolisR_ for R and _gibbC_ and _rwMetropolisC_ for C++.

The R package 'microbenchmark' can be used to benchmark the above R and C++ functions.

Besides, __StatComp22015__ also includes how my simulation implement and graphic results. The plot procedure called _plot_s_ is finished by R and the calculation is originally finished by Python which you can find [here](https://github.com/zyyfor2000/code). I translate the the calculation into R language called _sim_result_ with _rtrunc_ for truncated distribution.


```{r,eval=TRUE}
library(StatComp22015)
library(microbenchmark)
```

# Compare between R and R++ {#Compare}

## Benchmarking _gibbsR_ and _gibbsC_

The source R code for _gibbsR_ is as follows:
```{r,eval=FALSE}
gibbsR = function(N,burn,thin,mu1,mu2,sigma1,sigma2,rho) {
  mat = matrix(0,nrow = N, ncol = 2)
  mat[1,] = c(mu1,mu2)
  s1 = sqrt(1-rho^2)*sigma1
  s2 = sqrt(1-rho^2)*sigma2
  for (i in 2:N) {
    for (j in 1:thin) {
      y = mat[i-1, 2]
      m1 = mu1 + rho * (y - mu2) * sigma1/sigma2
      x = rnorm(1, m1, s1)
      m2 = mu2 + rho * (x - mu1) * sigma2/sigma1
      y = rnorm(1, m2, s2)
    }
    mat[i, ] = c(x, y)
  }
  mat[(burn+1):N,]
}
```

The above R code involves two loops, which could be very slow even for R-3.01 or any higher version. The corresponding C++ code is as follows.

```{r,eval=FALSE}
NumericMatrix gibbsC(int N,int burn, int thin, double mu1, double mu2,double sigma1,double sigma2,double rho) {
  NumericMatrix mat(N, 2);
  double m1,m2;
  double x = mu1, y = mu2;
  mat(0,0) = x;
  mat(0,1) = y;
  double s1 = sqrt(1-pow(rho,2))*sigma1;
  double s2 = sqrt(1-pow(rho,2))*sigma2;
  for(int i = 1; i < N; i++) {
    for(int j = 0; j < thin; j++) {
      y = mat(j-1, 2);
      m1 = mu1 + rho * (y - 0) * sigma1/sigma2;
      x = rnorm(1, m1, s1)[0];
      m2 = mu2 + rho * (x - 0) * sigma2/sigma1;
      y = rnorm(1, m2, s2)[0];
    }
    mat(i, 0) = x;
    mat(i, 1) = y;
  }
  NumericMatrix mlast = mat( Range(burn,N-1) ,Range(0,1));
  return(mlast);
}
```

The R code for benchmarking _gibbsR_ and _gibbsC_ is as follows.

```{r,eval=TRUE}
tm1 = microbenchmark(
  gR = gibbsR(1000,100,1,0,0,1,1,0.9),
  gC = gibbsC(1000,100,1,0,0,1,1,0.9)
)
knitr::kable(summary(tm1)[,c(1,3,5,6)])
```

The results again show an evident computational speed gain of C++ against R.

## Benchmarking _rwMetropolisR_ and _rwMetropolisC_

The source R code for _rwMetropolisR_ is as follows:
```{r,eval=FALSE}
rwMetropolisR = function(n, sigma, x0, N, burn) {
    x = numeric(N)
    x[1] = x0
    u = runif(N)
    for (i in 2:N) {
    y = rnorm(1, x[i-1], sigma)
    if (u[i] <= (dt(y, n) / dt(x[i-1], n)))
      x[i] = y  
    else {
      x[i] = x[i-1]
    }
  }
  return(x[(burn+1):N])
}
```

The corresponding C++ code is as follows.

```{r,eval=FALSE}
NumericVector rwMetropolisC(int n, double sigma, double x0, int N, int burn) {
  NumericVector x(N);
  x[1] = x0;
  for (int i = 1; i < N; i++) {
    double u = runif(1)[0];
    double y = rnorm(1, x[i-1], sigma)[0];
    double tmp1 = ::Rf_dt(y, n, 0) ;
    double tmp2 = ::Rf_dt(x[i-1], n, 0);
    double tmp = tmp1/tmp2;
    if (u <= tmp){
      x[i] = y  ;
    }
      else {
        x[i] = x[i-1];
      }
  }
  NumericVector xlast = x[Range(burn,N-1)];
  return(xlast);
}
}
```

The R code for benchmarking _rwMetropolisR_ and _rwMetropolisC_ is as follows.

```{r,eval=TRUE}
tm1 = microbenchmark(
  rR = rwMetropolisR(2,1,1,1000,100),
  rC = rwMetropolisC(2,1,1,1000,100)
)
knitr::kable(summary(tm1)[,c(1,3,5,6)])
```

The results again show an evident computational speed gain of C++ against R.

[Back to the Content](#Content)

# Simulation for the results in the paper {#Simulation}

## How to manipulate the 3 methods proposed in the paper and 6 nonparametric methods 

In the paper called _A general method for addressing forecasting uncertainty in inventory models_, 3 methods have been proposed to solve the question below.
$$
\begin{equation}
	TC(S_n) = h*E(S_n -D_{[n+1,n+L]})^+ +b*E(S_n -D_{[n+1,n+L]})^-
\end{equation}
$$

All the three methods are about parameter methods. Then  I apply 6 methods about nonparameter methods to compare.

Our goalï¼š
$$
F_{D_{[n+1, n+L]}}(S) = \dfrac{b}{b+h}
$$
Some simple description about 3 method in the paper.

- Method 1

$$
\begin{equation}
\begin{aligned}
	\hat{\mu}&=\frac{1}{n} \sum_{t=1}^{n} D_{t}\\
	\hat{\sigma}^2&=\frac{1}{n-1} \sum_{t=1}^n\left(D_t-\hat{\mu}\right)^2	\\
\end{aligned}
\end{equation}
$$

- Method 2

$$
\begin{equation}
	\label{q1}
\begin{aligned}
	\mu&=\hat{\mu}+\frac{\sigma Z}{\sqrt{n}}\\
	\sigma^{2}&=\frac{(n-1) \hat{\sigma}^{2}}{X}\\
	Z&\sim N(0,1),X\sim \mathcal{\chi}^2_{n-1}
\end{aligned}
\end{equation}
$$

- Method 3

$$
\begin{equation}
\begin{aligned}			
	\mu& \approx \hat{\mu}+\sqrt{\frac{\hat{\sigma}^{2}}{n}} Z_{1}\\		
	\sigma^{2}&\approx\hat{\sigma}^{2}+\sqrt{\frac{2 \hat{\sigma}^{4}}{n}}Z_{2}
\end{aligned}
\end{equation}
$$

We need to use a truncated normal distribution with appropriately bounded support to model $Z_2$. So the _rtrunc_ for truncated distribution is used.

The source R code for _rtrunc_ is as follows:

```{r,eval=FALSE}
rtrunc = function(n, distr, lower = -Inf, upper = Inf, ...){
  makefun = function(prefix, FUN, ...){
    txt = paste(prefix, FUN, "(x, ...)", sep = "")
    function(x, ...) eval(parse(text = txt))
  }
  if(length(n) > 1) n = length(n)
  pfun = makefun("p", distr, ...)
  qfun = makefun("q", distr, ...)
  lo = pfun(lower, ...)
  up = pfun(upper, ...)
  u = runif(n, lo, up)
  return(qfun(u, ...))
}
```

For example, If we want generate 10 random numbers from a truncated normal distribution truncated by left 0 with mean = 2, sd = 5, the code below can be used.

```{r}
x = rtrunc(10, "norm", lower = 0, mean = 2, sd = 5)
print(x)
```

Then I add 6 common nonparametric methods where last three comes from the paper called _A new family of nonparametric quantile estimators_ to compare the methods above.

The source R code for _sim_result_ is as follows:


```{r}
sim_result = function(n_replication,n,sigma2){
  # parameter setting
  mu = 4
  b =50
  h = 1
  L = 5
  # population generating
  x_all = rnorm(n_replication,10,sqrt(sigma2))
  x_l_all = rep(0,n_replication)
  for (i in c(1:n_replication)){
    x_l_all[i] = sum(sample(x_all,5))
  }
  r_all = 100 # 100 data sets are simulated
  tc_all = array(data = 0, dim = c(r_all,3)) # 3 methods
  tc_np = array(data = 0, dim = c(r_all,6)) # 6 methods
  for (r in c(1:r_all)){
    data_known = sample(x_all,n)
    mu_hat = mean(data_known)
    sigma2_hat = var(data_known)
    # exact method
    z_exact = rep(0,n_replication)
    for (j in c(1:n_replication)){
      sigma2_hat_1 = (n-1)*sigma2_hat/rchisq(1,n-1)
      mu_hat_1 = mu_hat + sqrt(sigma2_hat_1)*rnorm(1)/sqrt(n)
      z_exact[j] = rnorm(1,L*mu_hat_1,sqrt(L*sigma2_hat_1))
    }
    S_exact = quantile(z_exact,b/(b+h))
    z_exact_new = S_exact - x_l_all
    TC_exact = (sum(h * z_exact_new[z_exact_new > 0]) -
                  sum(b * z_exact_new[z_exact_new < 0])) / n_replication
    tc_all[r,1] = TC_exact
    # classic method
    S_classic = L * mu_hat + sqrt(L) * (-qnorm(b / (b + h))) * sqrt(sigma2_hat)
    z_classic = rnorm(n_replication, L * mu_hat, sqrt(L * sigma2_hat))
    z_classic_new = S_classic - x_l_all
    TC_classic = (sum(h * z_classic_new[z_classic_new > 0]) -
                    sum(b * z_classic_new[z_classic_new < 0])) / n_replication
    tc_all[r, 2] = TC_classic
    # asymptotic method
    z_asymtotic = rep(0,n_replication)
    left_bound = -sqrt(n / 2)
    for (j in c(1:n_replication)){
      sigma2_hat_1 = sigma2_hat + sqrt(2 * sigma2_hat ** 2 / n) * rtrunc(1, "norm",left_bound, Inf,  mean = 2, sd = 1)
      mu_hat_1 = mu_hat + sqrt(sigma2_hat / n) * rnorm(1)
      z_asymtotic[j] = rnorm(1,L * mu_hat_1,sqrt(L * sigma2_hat_1))
    }
    S_asymtotic = quantile(z_asymtotic, b / (b + h))
    z_asymtotic_new = S_asymtotic - x_l_all
    TC_asymtotic = (sum(h * z_asymtotic_new[z_asymtotic_new > 0]) -
                      sum(b * z_asymtotic_new[z_asymtotic_new < 0])) / n_replication
    tc_all[r, 3] = TC_asymtotic
    #s1
    S_1 = quantile(data_known, b / (b + h))*L
    z_1 = S_1 - x_l_all
    TC_1 = (sum(h * z_1[z_1 > 0]) -
              sum(b * z_1[z_1 < 0])) / n_replication
    tc_np[r, 1] = TC_1
    #s2
    n_1 = as.integer(floor(n * 0.5))
    n_2 = as.integer(floor(n * 0.8))
    S = rep(0,n_replication)
    for (j in c(1:100)){
      S[j] = quantile(sample(data_known, n_1, replace=TRUE), b / (b + h))
    }
    S_21 = mean(S)*L
    z_21 = S_21 - x_l_all
    TC_21 = (sum(h * z_21[z_21 > 0]) -sum(b * z_21[z_21 < 0])) / n_replication
    tc_np[r, 2] = TC_21
    S = rep(0,n_replication)
    for (j in c(1:100)){
      S[j] = quantile(sample(data_known, n_2, replace=TRUE), b / (b + h))
    }
    S_22 = mean(S)*L
    z_22 = S_22 - x_l_all
    TC_22 = (sum(h * z_22[z_22 > 0]) - sum(b * z_22[z_22 < 0])) / n_replication
    tc_np[r, 3] = TC_22
    # 3. SVP1-3
    data_ordered = sort(data_known)
    s_1 = 0
    p = b / (b + h)
    for (i in 2:(n-1)){
      s_1 = s_1 + (dbinom(i, n, p)+dbinom(i-1, n, p)) / 2 * data_ordered[i]
    }
    S_31 = (2*dbinom(0,n,p)+dbinom(1,n,p))/2*data_ordered[1]+dbinom(0,n,p)/2*data_ordered[2]-dbinom(0,n,p)/2*data_ordered[3]+s_1-dbinom(n,n,p)/2*data_ordered[n-2]+dbinom(n,n,p)/2*data_ordered[n-1]+(2*dbinom(n,n,p)+dbinom(n-1,n,p))/2*data_ordered[n]
    z_31 = S_31*L - x_l_all
    TC_31 = (sum(h * z_31[z_31 > 0]) - sum(b * z_31[z_31 < 0])) / n_replication
    tc_np[r, 4] = TC_31
    s_2 = 0
    for (i in 0:(n-1)) {
      s_2 = s_2 + dbinom(i,n,p)*data_ordered[i+1]
    }
    S_32 = s_2 +(2*data_ordered[n]-data_ordered[n-1])*dbinom(n,n,p)
    z_32 = S_32*L - x_l_all
    TC_32 = (sum(h * z_32[z_32 > 0]) - sum(b * z_32[z_32 < 0])) / n_replication
    tc_np[r, 5] = TC_32
    s_3 = 0
    for (i in 1:n){
      s_3 = s_3 + dbinom(i,n,p)*data_ordered[i]
    }
    S_33 = s_3 +(2*data_ordered[1]-data_ordered[2])*dbinom(0,n,p)
    z_33 = S_33*L - x_l_all
    TC_33 = (sum(h * z_33[z_33 > 0]) - sum(b * z_33[z_33 < 0])) / n_replication
    tc_np[r,6] = TC_33
  }
  result1 = as.vector(apply(tc_all,2,mean))
  result2 = as.vector(apply(tc_np, 2,mean))
  return(c(result1,result2))
}
```


The whole procedure is time-consuming when replication equals to 1000000 as the article sets. As a result, we let replication equals to 1000 just for show.

```{r,eval=TRUE}
begin = Sys.time()
n_total = c(5,10,20,100)
result = array(data = 0, dim = c(4,9))
n_replication = 1000
sigma2 = 1
for (i in 1:4){
    a = sim_result(n_replication,n_total[i],sigma2)
    result[i,] = as.matrix(a)
  }
end = Sys.time()
print(end-begin)
print(result)
```

## Graphical displays of my simulation result: _plot_s_




Since the whole running time for my simulation is very long, I import the results produced by Server in our group to present graphical displays of the table.

The source R code for _plot_s_ is as follows:

```{r,eval=FALSE}
plot_s = function(counter,result){
  sigma = c(1:10)
  par(mai = c(0.8,0.8,0.8,1))
  n_total = c(5,10,20,100)
  plot(c(1:9),result[1,counter,],type = "l",col=1,ylim=c(min(result[,counter,]-5),max(result[,counter,])+5),ylab = "total cost" ,xlab = "method",main = bquote(n == .(n_total[counter])))
  axis(1,"m",at = seq(1,9,1),labels = TRUE)
  for (i in 2:10){
    lines(result[i,counter,],type = "l",col=i)
  }
  usr = par("usr")
  x = usr[2]*1.02
  y = usr[4]*0.9
  legend(x,y, legend = sigma,lty=1,col = 1:10,ncol = 1,title = expression(sigma^2),xpd = TRUE)
}
```

Let's see how my simulation takes on.

```{r}
# my results are imported as data existed
result = array(data = 0, dim = c(10,4,9))
for (i in 1:10){
  a = read.table(paste("../data/text",i,".txt",sep = ""))
  a = a[,c(2,1,3,4,5,6,7,8,9)]
  result[i,,] = as.matrix(a)
}

```


```{r}
plot_s(1,result)
```

```{r}
plot_s(2,result)
```

```{r}
plot_s(3,result)
```

```{r}
plot_s(4,result)
```


[Back to the Content](#Content)

------------------------------------------------------------------------

<center><font color=black size=5>**This is the end of my introduction for the package, thank
you for reading!**</font></center>

------------------------------------------------------------------------